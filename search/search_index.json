{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"yapp \u2014 Yet Another Python (data) Pipeline yapp is a simple python data pipeline framework, inspired by ploomber . yapp allows you to quickly define pipelines to pack together your messy Python experiments and make better use of them. Code is hosted on GitHub . Warning Development is still in early stages, many things may change in the future. If you are looking for a similar tool to use right away in something resembling a production environment, just use Ploomber. yapp strives to be as simple as possible and make you focus on the correctness of your algorithms. It's developed with specific requirements in mind and built according to those: it may be the best choice for you once completed, or may be not. Basic functionality is there but there are some very rough edges to be smoothed. There are no tests and not even a proper example yet. Install pip install yapp-pipelines Usage To use yapp-pipelines you need two things: A directory containing your code A pipelines.yml file inside it Pipelines configuration is defined in a pipelines.yml , it contains all the steps to perform in each of your pipelines and all the needed resources. A Pipeline is made up of Jobs. A Job represents a step of the pipeline, it takes inputs as parameters and returns a dict of outputs. A Job can a class that extends yapp.Job , or just a function returning a dictionary. The configuration defines all the dependencies of every Job in the Pipeline. They are resolved and then run one at the time (even if it may be possible to run them in parallel, this is a willingly design choice). Pipelines can have hooks to perform specific task before or after each task (such as updating some kind of status monitor) For a complete overview on how to define pipelines se the documentation in the wiki. You can then run your pipeline with yapp [pipeline name] . The yapp command You can run a pipeline using the yapp command: yapp [-h] [-p [PATH]] [-d] pipeline yapp automatically searches for classes and functions you use in your yaml files. It searches in, in order: The pipeline directory (if it exists) Top level directory of your code yapp built-in modules The first two are relative to the current working directory or to the supplied using path or -p Planned features and changes: new detailed pipelines.yml specification A good and working example Better comprehensive docstrings Better tests Pipeline status monitor class with an example Allow importing from Jupyter notebooks Inputs aliases Consider permitting repeted tasks in a single pipeline (can this be useful?) For each step, keep track of the inputs required in future steps. So that unneeded ones can be removed from memory Graph data flow between jobs Example Code Structure example_project/ \u251c\u2500\u2500my_pipeline/ # pipeline directory \u2502 \u251c\u2500\u2500MainTask.py # MainTask class inside a bad CamelCase filename \u2502 \u251c\u2500\u2500another_task.py # AnotherTask inside a better snake_case filename \u2502 \u2514\u2500\u2500... \u251c\u2500\u2500data/ \u2502 \u251c\u2500\u2500raw.csv # Input data \u2502 \u2514\u2500\u2500... \u2514\u2500\u2500pipelines.yml pipelines.yml my_pipeline : # define a new pipeline inputs : - file.CsvInput : # use input from CSV files directory : \"./data\" expose : - file.CsvInput : - \"raw.csv\" : raw_data # Read \"raw.csv\" and use it as the raw_data input steps : - MainTask : - AnotherTask : MainTask # AnotherTask must be run after MainTask","title":"Introduction"},{"location":"#yapp-yet-another-python-data-pipeline","text":"yapp is a simple python data pipeline framework, inspired by ploomber . yapp allows you to quickly define pipelines to pack together your messy Python experiments and make better use of them. Code is hosted on GitHub . Warning Development is still in early stages, many things may change in the future. If you are looking for a similar tool to use right away in something resembling a production environment, just use Ploomber. yapp strives to be as simple as possible and make you focus on the correctness of your algorithms. It's developed with specific requirements in mind and built according to those: it may be the best choice for you once completed, or may be not. Basic functionality is there but there are some very rough edges to be smoothed. There are no tests and not even a proper example yet.","title":"yapp \u2014 Yet Another Python (data) Pipeline"},{"location":"#install","text":"pip install yapp-pipelines","title":"Install"},{"location":"#usage","text":"To use yapp-pipelines you need two things: A directory containing your code A pipelines.yml file inside it Pipelines configuration is defined in a pipelines.yml , it contains all the steps to perform in each of your pipelines and all the needed resources. A Pipeline is made up of Jobs. A Job represents a step of the pipeline, it takes inputs as parameters and returns a dict of outputs. A Job can a class that extends yapp.Job , or just a function returning a dictionary. The configuration defines all the dependencies of every Job in the Pipeline. They are resolved and then run one at the time (even if it may be possible to run them in parallel, this is a willingly design choice). Pipelines can have hooks to perform specific task before or after each task (such as updating some kind of status monitor) For a complete overview on how to define pipelines se the documentation in the wiki. You can then run your pipeline with yapp [pipeline name] .","title":"Usage"},{"location":"#the-yapp-command","text":"You can run a pipeline using the yapp command: yapp [-h] [-p [PATH]] [-d] pipeline yapp automatically searches for classes and functions you use in your yaml files. It searches in, in order: The pipeline directory (if it exists) Top level directory of your code yapp built-in modules The first two are relative to the current working directory or to the supplied using path or -p","title":"The yapp command"},{"location":"#planned-features-and-changes","text":"new detailed pipelines.yml specification A good and working example Better comprehensive docstrings Better tests Pipeline status monitor class with an example Allow importing from Jupyter notebooks Inputs aliases Consider permitting repeted tasks in a single pipeline (can this be useful?) For each step, keep track of the inputs required in future steps. So that unneeded ones can be removed from memory Graph data flow between jobs","title":"Planned features and changes:"},{"location":"#example","text":"Code Structure example_project/ \u251c\u2500\u2500my_pipeline/ # pipeline directory \u2502 \u251c\u2500\u2500MainTask.py # MainTask class inside a bad CamelCase filename \u2502 \u251c\u2500\u2500another_task.py # AnotherTask inside a better snake_case filename \u2502 \u2514\u2500\u2500... \u251c\u2500\u2500data/ \u2502 \u251c\u2500\u2500raw.csv # Input data \u2502 \u2514\u2500\u2500... \u2514\u2500\u2500pipelines.yml pipelines.yml my_pipeline : # define a new pipeline inputs : - file.CsvInput : # use input from CSV files directory : \"./data\" expose : - file.CsvInput : - \"raw.csv\" : raw_data # Read \"raw.csv\" and use it as the raw_data input steps : - MainTask : - AnotherTask : MainTask # AnotherTask must be run after MainTask","title":"Example"},{"location":"configuration/","text":"Configuration Configuraiton is done using a pipelines.yml file. Top level keys are names of pipelines or +all , used to define a global configuration. Warning pipelines.yml specification can change anytime for 0.x.x versions. Note I wanted to write down a proper YAML schema at first but ended up with this quick notation, mainly 'cause I'm lazy. Every tag starting with \"$\" is a generic placeholder, which type is later specified. version 0.1.x $pipeline : inputs : # optional - $adapter outputs : # optional - $adapter expose : # optional - $expose_map hooks : # optional on_pipeline_start : - $hook on_job_start : - $hook on_job_finish : - $hook on_pipeline_finish : - $hook steps : # required - $step $adapter : str or dict with the first key used as name and the value ignored, the others are passed as arguments to __init__ $hook : str $expose_map : dict with a single pair str , [list of dict with a single str key-value pair] $step : str or dict with a single str key-value pair, or a sigle str : list pair str used as $adapter , $hook and step should be valid Python module strings. Pipeline fields A pipeline can specify the following attributes, the only strictly required is steps . steps Used to define the jobs that make up the pipeline and their dependencies. Contains a list, each element represents a Job and its dependencies. inputs Used to define input sources. Contains a list, each element represents an InputAdapter and its required arguments. outputs Used to define outputs to write results to. Contains a list, each element represents an OutputAdapter and its required arguments. expose Used to define aliases for inputs. Contains a list, each element represents a list of inputs to be exposed from a source. hooks Used to define the hooks to perform at specific events. Contains a dict, with the keys being the possible events that trigger a hook. For each key a list of functions to use as hooks can be specified. Every event key is optional. Special types Types defined by yapp that can be used in pipelines.yml : !env Reads an environment variable. If there is a DATA_DIR environment variable, to which is assigned the value ../data/latest , the following: !env DATA_DIR is automatically replaced with ../data/latest .","title":"Configuration"},{"location":"configuration/#configuration","text":"Configuraiton is done using a pipelines.yml file. Top level keys are names of pipelines or +all , used to define a global configuration. Warning pipelines.yml specification can change anytime for 0.x.x versions. Note I wanted to write down a proper YAML schema at first but ended up with this quick notation, mainly 'cause I'm lazy. Every tag starting with \"$\" is a generic placeholder, which type is later specified. version 0.1.x $pipeline : inputs : # optional - $adapter outputs : # optional - $adapter expose : # optional - $expose_map hooks : # optional on_pipeline_start : - $hook on_job_start : - $hook on_job_finish : - $hook on_pipeline_finish : - $hook steps : # required - $step $adapter : str or dict with the first key used as name and the value ignored, the others are passed as arguments to __init__ $hook : str $expose_map : dict with a single pair str , [list of dict with a single str key-value pair] $step : str or dict with a single str key-value pair, or a sigle str : list pair str used as $adapter , $hook and step should be valid Python module strings.","title":"Configuration"},{"location":"configuration/#pipeline-fields","text":"A pipeline can specify the following attributes, the only strictly required is steps .","title":"Pipeline fields"},{"location":"configuration/#steps","text":"Used to define the jobs that make up the pipeline and their dependencies. Contains a list, each element represents a Job and its dependencies.","title":"steps"},{"location":"configuration/#inputs","text":"Used to define input sources. Contains a list, each element represents an InputAdapter and its required arguments.","title":"inputs"},{"location":"configuration/#outputs","text":"Used to define outputs to write results to. Contains a list, each element represents an OutputAdapter and its required arguments.","title":"outputs"},{"location":"configuration/#expose","text":"Used to define aliases for inputs. Contains a list, each element represents a list of inputs to be exposed from a source.","title":"expose"},{"location":"configuration/#hooks","text":"Used to define the hooks to perform at specific events. Contains a dict, with the keys being the possible events that trigger a hook. For each key a list of functions to use as hooks can be specified. Every event key is optional.","title":"hooks"},{"location":"configuration/#special-types","text":"Types defined by yapp that can be used in pipelines.yml :","title":"Special types"},{"location":"configuration/#env","text":"Reads an environment variable. If there is a DATA_DIR environment variable, to which is assigned the value ../data/latest , the following: !env DATA_DIR is automatically replaced with ../data/latest .","title":"!env"},{"location":"reference/","text":"Reference yapp.core yapp core classes AttrDict Bases: dict Extends dict so that elements can be accessed as attributes __init__ ( other = None , ** kwargs ) Create a new AttrDict Parameters: Name Type Description Default self None, mapping , iterable required recursive_convert ( obj ) staticmethod Makes all dict inside obj AttrDict InputAdapter Bases: ABC Abstract Input Adapter An input adapter represents a type of input from a specific source get ( key ) abstractmethod Returns the requested input Inputs Bases: dict Inputs implementation (just dict with some utility methods) expose ( source , internal_name , name ) Expose input attribute using another name register ( name , adapter ) New input adapter (just a new Item) Job Bases: ABC Job represents a step in our pipeline config () property Shortcut for self.pipeline.config execute ( * inputs ) abstractmethod Job entrypoint name () property Helper to return the name of the class OutputAdapter Bases: ABC Abstract output Adapter An output adapter represents a specific output destination save ( key , data ) abstractmethod Save data here Pipeline yapp Pipeline object Pipeline implementation. Collects jobs, hooks and input and output adapter and runs the pipeline. Attributes: Name Type Description OK_LOGLEVEL int Loglevel to use for pipeline and jobs completed execution status messages VALID_HOOKS list list of valid hooks that can be used in a pipeline __nested_timed_calls int level of nested calls to timed , used to enhance logging __call__ ( inputs = None , outputs = None , config = None ) Pipeline entrypoint Sets up inputs, outputs and config (if specified) and runs the pipeline _run () Runs all Pipeline's jobs _run_job ( job ) Execution of a single job config () property Shortcut for configuration from inputs job_name () property Shortcut for self.current_job.name which handles no current_job run_hook ( hook_name ) Run all hooks for current event A hook is just a function taking a pipeline as single argument Parameters: Name Type Description Default hook_name str name of the hook to run (\"on_pipeline_start\", \"on_job_start\", etc.) required save_output ( name , data ) Save data to each output adapter Parameters: Name Type Description Default name name to pass to the output adapters when saving the data required data data to save required timed ( typename , name , func , * args , ** kwargs ) Runs a timed execution of a function, logging times The first two parameters are used to specify the type and name of the entity to run. Parameters: Name Type Description Default typename str name of the type of the component to run (\"pipeline\", \"job\", \"hook\", etc.) required name str name of the component to run required func callable function to run required *args () **kwargs {} Returns: Type Description (Any) The output of provided function attr_dict AttrDict Bases: dict Extends dict so that elements can be accessed as attributes __init__ ( other = None , ** kwargs ) Create a new AttrDict Parameters: Name Type Description Default self None, mapping , iterable required recursive_convert ( obj ) staticmethod Makes all dict inside obj AttrDict errors ConfigurationError Bases: RuntimeError Exception raised when an invalid configuration file is found input_adapter InputAdapter Bases: ABC Abstract Input Adapter An input adapter represents a type of input from a specific source get ( key ) abstractmethod Returns the requested input inputs Inputs Bases: dict Inputs implementation (just dict with some utility methods) expose ( source , internal_name , name ) Expose input attribute using another name register ( name , adapter ) New input adapter (just a new Item) job Job Bases: ABC Job represents a step in our pipeline config () property Shortcut for self.pipeline.config execute ( * inputs ) abstractmethod Job entrypoint name () property Helper to return the name of the class output_adapter OutputAdapter Bases: ABC Abstract output Adapter An output adapter represents a specific output destination save ( key , data ) abstractmethod Save data here pipeline Pipeline yapp Pipeline object Pipeline implementation. Collects jobs, hooks and input and output adapter and runs the pipeline. Attributes: Name Type Description OK_LOGLEVEL int Loglevel to use for pipeline and jobs completed execution status messages VALID_HOOKS list list of valid hooks that can be used in a pipeline __nested_timed_calls int level of nested calls to timed , used to enhance logging __call__ ( inputs = None , outputs = None , config = None ) Pipeline entrypoint Sets up inputs, outputs and config (if specified) and runs the pipeline _run () Runs all Pipeline's jobs _run_job ( job ) Execution of a single job config () property Shortcut for configuration from inputs job_name () property Shortcut for self.current_job.name which handles no current_job run_hook ( hook_name ) Run all hooks for current event A hook is just a function taking a pipeline as single argument Parameters: Name Type Description Default hook_name str name of the hook to run (\"on_pipeline_start\", \"on_job_start\", etc.) required save_output ( name , data ) Save data to each output adapter Parameters: Name Type Description Default name name to pass to the output adapters when saving the data required data data to save required timed ( typename , name , func , * args , ** kwargs ) Runs a timed execution of a function, logging times The first two parameters are used to specify the type and name of the entity to run. Parameters: Name Type Description Default typename str name of the type of the component to run (\"pipeline\", \"job\", \"hook\", etc.) required name str name of the component to run required func callable function to run required *args () **kwargs {} Returns: Type Description (Any) The output of provided function yapp.cli yapp cli parsing main () yapp cli entrypoint logs LogFormatter Bases: logging . Formatter Custom LogFormatter, probably not the best way at all to do this but was fun doing it this way. get_color ( loglevel = None ) Returns the color escape characters to print add_logging_level ( level_name , level_num , method_name = None ) Comprehensively adds a new logging level to the logging module and the currently configured logging class. level_name becomes an attribute of the logging module with the value level_num . method_name becomes a convenience method for both logging itself and the class returned by logging.getLoggerClass() (usually just logging.Logger ). If method_name is not specified, level_name.lower() is used. To avoid accidental clobberings of existing attributes, this method will raise an AttributeError if the level name is already an attribute of the logging module or if the method name is already present Taken from this great answer on StackOverflow Example add_logging_level('TRACE', logging.DEBUG - 5) logging.getLogger( name ).setLevel(\"TRACE\") logging.getLogger( name ).trace('that worked') logging.trace('so did this') logging.TRACE 5 setup_logging ( loglevel , color = False , logfile = '' , show_lineno = False ) Setup logging for yapp parsing ConfigParser Parses config files and build a pipeline accordingly build_hooks ( cfg_hooks ) Sets up hooks from hooks field in YAML files build_inputs ( cfg_inputs , cfg_expose ) Sets up inputs from inputs and expose fields in YAML files build_job ( step ) Create Job given pipeline and step name build_outputs ( cfg_outputs ) Sets up outputs from outputs field in YAML files build_pipeline ( pipeline_cfg , inputs = None , outputs = None , hooks = None ) Creates pipeline from pipeline and config definition dicts create_adapter ( adapter_def ) Loads the relevant module and instantiates an adapter from it load_module ( module_name ) Loads a python module from a .py file or yapp modules parse () Reads and parses pipelines.yml, creates a pipeline object switch_workdir ( workdir = None ) Switches to the pipeline workdir that jobs and hooks expect env_constructor ( loader , node ) Conctructor to automatically look up for env variables yaml_read ( path ) Read YAML from path validation ErrorHandler Bases: BasicErrorHandler Cerberus custom ErrorHandler to print config errors the way I want check_adapter ( field , value , error ) Ugly \"adapter\" definition checker check_code_reference ( field , value , error ) Check if a string can be a valid python module or function reference check_expose ( field , value , error ) Ugly \"expose\" definition checker check_step ( field , value , error ) Ugly \"step\" definition checker validate ( definitions ) Validate schema for definitions from YAML file yapp.adapters yapp input and ouput adapters file CsvInput Bases: InputAdapter CSV Input adapter An input adapter for CSV files, input is read into a pandas DataFrame pandas FunctionWrapperInputAdapter Bases: InputAdapter Very hacky generic input adapter pgsql PgSqlInput Bases: SqlInput Very simple PostgreSQL input adapter PgSqlOutput Bases: SqlOutput Very simple PostgreSQL ouput adapter snowflake SnowflakeInput Bases: SqlInput Very simple PostgreSQL input adapter sql SqlInput Bases: InputAdapter SQL Input adapter An input adapter for SQL databases, input is read into a pandas DataFrame SqlOutput Bases: OutputAdapter SQL output adapter Output adapter for SQL databases, a pandas DataFrame is written to a table utils DummyInput Bases: InputAdapter Dummy input adapter that always returns an empty DataFrame DummyOutput Bases: OutputAdapter Dummy output adapter that prints data it should save","title":"Reference"},{"location":"reference/#reference","text":"","title":"Reference"},{"location":"reference/#yapp.core","text":"yapp core classes","title":"core"},{"location":"reference/#yapp.core.AttrDict","text":"Bases: dict Extends dict so that elements can be accessed as attributes","title":"AttrDict"},{"location":"reference/#yapp.core.attr_dict.AttrDict.__init__","text":"Create a new AttrDict Parameters: Name Type Description Default self None, mapping , iterable required","title":"__init__()"},{"location":"reference/#yapp.core.attr_dict.AttrDict.recursive_convert","text":"Makes all dict inside obj AttrDict","title":"recursive_convert()"},{"location":"reference/#yapp.core.InputAdapter","text":"Bases: ABC Abstract Input Adapter An input adapter represents a type of input from a specific source","title":"InputAdapter"},{"location":"reference/#yapp.core.input_adapter.InputAdapter.get","text":"Returns the requested input","title":"get()"},{"location":"reference/#yapp.core.Inputs","text":"Bases: dict Inputs implementation (just dict with some utility methods)","title":"Inputs"},{"location":"reference/#yapp.core.inputs.Inputs.expose","text":"Expose input attribute using another name","title":"expose()"},{"location":"reference/#yapp.core.inputs.Inputs.register","text":"New input adapter (just a new Item)","title":"register()"},{"location":"reference/#yapp.core.Job","text":"Bases: ABC Job represents a step in our pipeline","title":"Job"},{"location":"reference/#yapp.core.job.Job.config","text":"Shortcut for self.pipeline.config","title":"config()"},{"location":"reference/#yapp.core.job.Job.execute","text":"Job entrypoint","title":"execute()"},{"location":"reference/#yapp.core.job.Job.name","text":"Helper to return the name of the class","title":"name()"},{"location":"reference/#yapp.core.OutputAdapter","text":"Bases: ABC Abstract output Adapter An output adapter represents a specific output destination","title":"OutputAdapter"},{"location":"reference/#yapp.core.output_adapter.OutputAdapter.save","text":"Save data here","title":"save()"},{"location":"reference/#yapp.core.Pipeline","text":"yapp Pipeline object Pipeline implementation. Collects jobs, hooks and input and output adapter and runs the pipeline. Attributes: Name Type Description OK_LOGLEVEL int Loglevel to use for pipeline and jobs completed execution status messages VALID_HOOKS list list of valid hooks that can be used in a pipeline __nested_timed_calls int level of nested calls to timed , used to enhance logging","title":"Pipeline"},{"location":"reference/#yapp.core.pipeline.Pipeline.__call__","text":"Pipeline entrypoint Sets up inputs, outputs and config (if specified) and runs the pipeline","title":"__call__()"},{"location":"reference/#yapp.core.pipeline.Pipeline._run","text":"Runs all Pipeline's jobs","title":"_run()"},{"location":"reference/#yapp.core.pipeline.Pipeline._run_job","text":"Execution of a single job","title":"_run_job()"},{"location":"reference/#yapp.core.pipeline.Pipeline.config","text":"Shortcut for configuration from inputs","title":"config()"},{"location":"reference/#yapp.core.pipeline.Pipeline.job_name","text":"Shortcut for self.current_job.name which handles no current_job","title":"job_name()"},{"location":"reference/#yapp.core.pipeline.Pipeline.run_hook","text":"Run all hooks for current event A hook is just a function taking a pipeline as single argument Parameters: Name Type Description Default hook_name str name of the hook to run (\"on_pipeline_start\", \"on_job_start\", etc.) required","title":"run_hook()"},{"location":"reference/#yapp.core.pipeline.Pipeline.save_output","text":"Save data to each output adapter Parameters: Name Type Description Default name name to pass to the output adapters when saving the data required data data to save required","title":"save_output()"},{"location":"reference/#yapp.core.pipeline.Pipeline.timed","text":"Runs a timed execution of a function, logging times The first two parameters are used to specify the type and name of the entity to run. Parameters: Name Type Description Default typename str name of the type of the component to run (\"pipeline\", \"job\", \"hook\", etc.) required name str name of the component to run required func callable function to run required *args () **kwargs {} Returns: Type Description (Any) The output of provided function","title":"timed()"},{"location":"reference/#yapp.core.attr_dict","text":"","title":"attr_dict"},{"location":"reference/#yapp.core.attr_dict.AttrDict","text":"Bases: dict Extends dict so that elements can be accessed as attributes","title":"AttrDict"},{"location":"reference/#yapp.core.attr_dict.AttrDict.__init__","text":"Create a new AttrDict Parameters: Name Type Description Default self None, mapping , iterable required","title":"__init__()"},{"location":"reference/#yapp.core.attr_dict.AttrDict.recursive_convert","text":"Makes all dict inside obj AttrDict","title":"recursive_convert()"},{"location":"reference/#yapp.core.errors","text":"","title":"errors"},{"location":"reference/#yapp.core.errors.ConfigurationError","text":"Bases: RuntimeError Exception raised when an invalid configuration file is found","title":"ConfigurationError"},{"location":"reference/#yapp.core.input_adapter","text":"","title":"input_adapter"},{"location":"reference/#yapp.core.input_adapter.InputAdapter","text":"Bases: ABC Abstract Input Adapter An input adapter represents a type of input from a specific source","title":"InputAdapter"},{"location":"reference/#yapp.core.input_adapter.InputAdapter.get","text":"Returns the requested input","title":"get()"},{"location":"reference/#yapp.core.inputs","text":"","title":"inputs"},{"location":"reference/#yapp.core.inputs.Inputs","text":"Bases: dict Inputs implementation (just dict with some utility methods)","title":"Inputs"},{"location":"reference/#yapp.core.inputs.Inputs.expose","text":"Expose input attribute using another name","title":"expose()"},{"location":"reference/#yapp.core.inputs.Inputs.register","text":"New input adapter (just a new Item)","title":"register()"},{"location":"reference/#yapp.core.job","text":"","title":"job"},{"location":"reference/#yapp.core.job.Job","text":"Bases: ABC Job represents a step in our pipeline","title":"Job"},{"location":"reference/#yapp.core.job.Job.config","text":"Shortcut for self.pipeline.config","title":"config()"},{"location":"reference/#yapp.core.job.Job.execute","text":"Job entrypoint","title":"execute()"},{"location":"reference/#yapp.core.job.Job.name","text":"Helper to return the name of the class","title":"name()"},{"location":"reference/#yapp.core.output_adapter","text":"","title":"output_adapter"},{"location":"reference/#yapp.core.output_adapter.OutputAdapter","text":"Bases: ABC Abstract output Adapter An output adapter represents a specific output destination","title":"OutputAdapter"},{"location":"reference/#yapp.core.output_adapter.OutputAdapter.save","text":"Save data here","title":"save()"},{"location":"reference/#yapp.core.pipeline","text":"","title":"pipeline"},{"location":"reference/#yapp.core.pipeline.Pipeline","text":"yapp Pipeline object Pipeline implementation. Collects jobs, hooks and input and output adapter and runs the pipeline. Attributes: Name Type Description OK_LOGLEVEL int Loglevel to use for pipeline and jobs completed execution status messages VALID_HOOKS list list of valid hooks that can be used in a pipeline __nested_timed_calls int level of nested calls to timed , used to enhance logging","title":"Pipeline"},{"location":"reference/#yapp.core.pipeline.Pipeline.__call__","text":"Pipeline entrypoint Sets up inputs, outputs and config (if specified) and runs the pipeline","title":"__call__()"},{"location":"reference/#yapp.core.pipeline.Pipeline._run","text":"Runs all Pipeline's jobs","title":"_run()"},{"location":"reference/#yapp.core.pipeline.Pipeline._run_job","text":"Execution of a single job","title":"_run_job()"},{"location":"reference/#yapp.core.pipeline.Pipeline.config","text":"Shortcut for configuration from inputs","title":"config()"},{"location":"reference/#yapp.core.pipeline.Pipeline.job_name","text":"Shortcut for self.current_job.name which handles no current_job","title":"job_name()"},{"location":"reference/#yapp.core.pipeline.Pipeline.run_hook","text":"Run all hooks for current event A hook is just a function taking a pipeline as single argument Parameters: Name Type Description Default hook_name str name of the hook to run (\"on_pipeline_start\", \"on_job_start\", etc.) required","title":"run_hook()"},{"location":"reference/#yapp.core.pipeline.Pipeline.save_output","text":"Save data to each output adapter Parameters: Name Type Description Default name name to pass to the output adapters when saving the data required data data to save required","title":"save_output()"},{"location":"reference/#yapp.core.pipeline.Pipeline.timed","text":"Runs a timed execution of a function, logging times The first two parameters are used to specify the type and name of the entity to run. Parameters: Name Type Description Default typename str name of the type of the component to run (\"pipeline\", \"job\", \"hook\", etc.) required name str name of the component to run required func callable function to run required *args () **kwargs {} Returns: Type Description (Any) The output of provided function","title":"timed()"},{"location":"reference/#yapp.cli","text":"yapp cli parsing","title":"cli"},{"location":"reference/#yapp.cli.main","text":"yapp cli entrypoint","title":"main()"},{"location":"reference/#yapp.cli.logs","text":"","title":"logs"},{"location":"reference/#yapp.cli.logs.LogFormatter","text":"Bases: logging . Formatter Custom LogFormatter, probably not the best way at all to do this but was fun doing it this way.","title":"LogFormatter"},{"location":"reference/#yapp.cli.logs.LogFormatter.get_color","text":"Returns the color escape characters to print","title":"get_color()"},{"location":"reference/#yapp.cli.logs.add_logging_level","text":"Comprehensively adds a new logging level to the logging module and the currently configured logging class. level_name becomes an attribute of the logging module with the value level_num . method_name becomes a convenience method for both logging itself and the class returned by logging.getLoggerClass() (usually just logging.Logger ). If method_name is not specified, level_name.lower() is used. To avoid accidental clobberings of existing attributes, this method will raise an AttributeError if the level name is already an attribute of the logging module or if the method name is already present Taken from this great answer on StackOverflow","title":"add_logging_level()"},{"location":"reference/#yapp.cli.logs.add_logging_level--example","text":"add_logging_level('TRACE', logging.DEBUG - 5) logging.getLogger( name ).setLevel(\"TRACE\") logging.getLogger( name ).trace('that worked') logging.trace('so did this') logging.TRACE 5","title":"Example"},{"location":"reference/#yapp.cli.logs.setup_logging","text":"Setup logging for yapp","title":"setup_logging()"},{"location":"reference/#yapp.cli.parsing","text":"","title":"parsing"},{"location":"reference/#yapp.cli.parsing.ConfigParser","text":"Parses config files and build a pipeline accordingly","title":"ConfigParser"},{"location":"reference/#yapp.cli.parsing.ConfigParser.build_hooks","text":"Sets up hooks from hooks field in YAML files","title":"build_hooks()"},{"location":"reference/#yapp.cli.parsing.ConfigParser.build_inputs","text":"Sets up inputs from inputs and expose fields in YAML files","title":"build_inputs()"},{"location":"reference/#yapp.cli.parsing.ConfigParser.build_job","text":"Create Job given pipeline and step name","title":"build_job()"},{"location":"reference/#yapp.cli.parsing.ConfigParser.build_outputs","text":"Sets up outputs from outputs field in YAML files","title":"build_outputs()"},{"location":"reference/#yapp.cli.parsing.ConfigParser.build_pipeline","text":"Creates pipeline from pipeline and config definition dicts","title":"build_pipeline()"},{"location":"reference/#yapp.cli.parsing.ConfigParser.create_adapter","text":"Loads the relevant module and instantiates an adapter from it","title":"create_adapter()"},{"location":"reference/#yapp.cli.parsing.ConfigParser.load_module","text":"Loads a python module from a .py file or yapp modules","title":"load_module()"},{"location":"reference/#yapp.cli.parsing.ConfigParser.parse","text":"Reads and parses pipelines.yml, creates a pipeline object","title":"parse()"},{"location":"reference/#yapp.cli.parsing.ConfigParser.switch_workdir","text":"Switches to the pipeline workdir that jobs and hooks expect","title":"switch_workdir()"},{"location":"reference/#yapp.cli.parsing.env_constructor","text":"Conctructor to automatically look up for env variables","title":"env_constructor()"},{"location":"reference/#yapp.cli.parsing.yaml_read","text":"Read YAML from path","title":"yaml_read()"},{"location":"reference/#yapp.cli.validation","text":"","title":"validation"},{"location":"reference/#yapp.cli.validation.ErrorHandler","text":"Bases: BasicErrorHandler Cerberus custom ErrorHandler to print config errors the way I want","title":"ErrorHandler"},{"location":"reference/#yapp.cli.validation.check_adapter","text":"Ugly \"adapter\" definition checker","title":"check_adapter()"},{"location":"reference/#yapp.cli.validation.check_code_reference","text":"Check if a string can be a valid python module or function reference","title":"check_code_reference()"},{"location":"reference/#yapp.cli.validation.check_expose","text":"Ugly \"expose\" definition checker","title":"check_expose()"},{"location":"reference/#yapp.cli.validation.check_step","text":"Ugly \"step\" definition checker","title":"check_step()"},{"location":"reference/#yapp.cli.validation.validate","text":"Validate schema for definitions from YAML file","title":"validate()"},{"location":"reference/#yapp.adapters","text":"yapp input and ouput adapters","title":"adapters"},{"location":"reference/#yapp.adapters.file","text":"","title":"file"},{"location":"reference/#yapp.adapters.file.CsvInput","text":"Bases: InputAdapter CSV Input adapter An input adapter for CSV files, input is read into a pandas DataFrame","title":"CsvInput"},{"location":"reference/#yapp.adapters.pandas","text":"","title":"pandas"},{"location":"reference/#yapp.adapters.pandas.FunctionWrapperInputAdapter","text":"Bases: InputAdapter Very hacky generic input adapter","title":"FunctionWrapperInputAdapter"},{"location":"reference/#yapp.adapters.pgsql","text":"","title":"pgsql"},{"location":"reference/#yapp.adapters.pgsql.PgSqlInput","text":"Bases: SqlInput Very simple PostgreSQL input adapter","title":"PgSqlInput"},{"location":"reference/#yapp.adapters.pgsql.PgSqlOutput","text":"Bases: SqlOutput Very simple PostgreSQL ouput adapter","title":"PgSqlOutput"},{"location":"reference/#yapp.adapters.snowflake","text":"","title":"snowflake"},{"location":"reference/#yapp.adapters.snowflake.SnowflakeInput","text":"Bases: SqlInput Very simple PostgreSQL input adapter","title":"SnowflakeInput"},{"location":"reference/#yapp.adapters.sql","text":"","title":"sql"},{"location":"reference/#yapp.adapters.sql.SqlInput","text":"Bases: InputAdapter SQL Input adapter An input adapter for SQL databases, input is read into a pandas DataFrame","title":"SqlInput"},{"location":"reference/#yapp.adapters.sql.SqlOutput","text":"Bases: OutputAdapter SQL output adapter Output adapter for SQL databases, a pandas DataFrame is written to a table","title":"SqlOutput"},{"location":"reference/#yapp.adapters.utils","text":"","title":"utils"},{"location":"reference/#yapp.adapters.utils.DummyInput","text":"Bases: InputAdapter Dummy input adapter that always returns an empty DataFrame","title":"DummyInput"},{"location":"reference/#yapp.adapters.utils.DummyOutput","text":"Bases: OutputAdapter Dummy output adapter that prints data it should save","title":"DummyOutput"},{"location":"roadmap/","text":"Ideas for a possible evolution in 1.x.x Current pipelines.yml specs are totally not good, lots of checks it conforms to the supposed schema are done by hand because it's difficult to enforce a proper schema onto it. A few relevant problems and ideas: dictionaries should not depend on key ordering hooks definition is bad Exposed variables are separated from inputs definition. This is not necessarly a bad thing but it probably makes less clear which inputs are used for which argument (very subjective). A slightly better approach could be: pipelines.yml specification $pipeline : # pipeline name or \"+all\" # optional inputs : - name : $input_name # name is optional source : $input $args expose : - $data : $alias - $input : $input_name # input_name is optional $args expose : - $data : $alias # optional outputs : - name : $output_name sink : $output $args # optional hooks : - run : $hook_func on : $hook_event # required steps : - $step or instead just do like Ploomber and define everything inside Jobs alt pipelines.yml specification $pipeline : # pipeline name or \"+all\" # optional inputs : - name : $input_name # name is optional source : $input $args # optional outputs : - name : $output_name sink : $output $args # optional hooks : - run : $hook_func on : $hook_event # required steps : - do : $step input : - from : $input_name use : $data as : $alias require : $other_steps One might also thing about a kind of hybrid approach: allowing to specify aliases for inputs inside the definition, that are automatically used inside the jobs, while also allowing to also just define them inside steps.","title":"Roadmap"},{"location":"roadmap/#ideas-for-a-possible-evolution-in-1xx","text":"Current pipelines.yml specs are totally not good, lots of checks it conforms to the supposed schema are done by hand because it's difficult to enforce a proper schema onto it. A few relevant problems and ideas: dictionaries should not depend on key ordering hooks definition is bad Exposed variables are separated from inputs definition. This is not necessarly a bad thing but it probably makes less clear which inputs are used for which argument (very subjective). A slightly better approach could be: pipelines.yml specification $pipeline : # pipeline name or \"+all\" # optional inputs : - name : $input_name # name is optional source : $input $args expose : - $data : $alias - $input : $input_name # input_name is optional $args expose : - $data : $alias # optional outputs : - name : $output_name sink : $output $args # optional hooks : - run : $hook_func on : $hook_event # required steps : - $step or instead just do like Ploomber and define everything inside Jobs alt pipelines.yml specification $pipeline : # pipeline name or \"+all\" # optional inputs : - name : $input_name # name is optional source : $input $args # optional outputs : - name : $output_name sink : $output $args # optional hooks : - run : $hook_func on : $hook_event # required steps : - do : $step input : - from : $input_name use : $data as : $alias require : $other_steps One might also thing about a kind of hybrid approach: allowing to specify aliases for inputs inside the definition, that are automatically used inside the jobs, while also allowing to also just define them inside steps.","title":"Ideas for a possible evolution in 1.x.x"}]}