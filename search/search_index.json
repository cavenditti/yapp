{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"yapp \u2014 Yet Another Python (data) Pipeline yapp is a simple python data pipeline framework, inspired by ploomber . yapp allows you to quickly define pipelines to pack together your messy Python experiments and make better use of them. Code is hosted on GitHub . Warning Development is still in early stages, many things may change in the future. If you are looking for a similar tool to use right away in something resembling a production environment, just use Ploomber. yapp strives to be as simple as possible and make you focus on the correctness of your algorithms. It's developed with specific requirements in mind and built according to those: it may be the best choice for you once completed, or may be not. Basic functionality is there but there are some very rough edges to be smoothed. There are no tests and not even a proper example yet. Install pip install yapp-pipelines Usage To use yapp-pipelines you need two things: A directory containing your code A pipelines.yml file inside it Pipelines configuration is defined in a pipelines.yml , it contains all the steps to perform in each of your pipelines and all the needed resources. A Pipeline is made up of Jobs. A Job represents a step of the pipeline, it takes inputs as parameters and returns a dict of outputs. A Job can a class that extends yapp.Job , or just a function returning a dictionary. The configuration defines all the dependencies of every Job in the Pipeline. They are resolved and then run one at the time (even if it may be possible to run them in parallel, this is a willingly design choice). Pipelines can have hooks to perform specific task before or after each task (such as updating some kind of status monitor) For a complete overview on how to define pipelines se the Configuration page . You can then run your pipeline with yapp [pipeline name] . The yapp command You can run a pipeline using the yapp command: yapp [-h] [-p [PATH]] [-d] pipeline yapp automatically searches for classes and functions you use in your yaml files. It searches in, in order: The pipeline directory (if it exists) Top level directory of your code yapp built-in modules The first two are relative to the current working directory or to the supplied using path or -p Example Code Structure example_project/ \u251c\u2500\u2500my_pipeline/ # pipeline directory \u2502 \u251c\u2500\u2500MainTask.py # MainTask class inside a bad CamelCase filename \u2502 \u251c\u2500\u2500another_task.py # AnotherTask inside a better snake_case filename \u2502 \u2514\u2500\u2500... \u251c\u2500\u2500data/ \u2502 \u251c\u2500\u2500raw.csv # Input data \u2502 \u2514\u2500\u2500... \u2514\u2500\u2500pipelines.yml pipelines.yml my_pipeline : # define a new pipeline inputs : - from : file.CsvInput # use input from CSV files wih : directory : \"./data\" expose : - use : \"raw.csv\" # Read \"raw.csv\"\u2026 as : raw_data # \u2026and use it as the raw_data input steps : - run : MainTask - run : AnotherTask after : MainTask # AnotherTask must run only after MainTask","title":"Introduction"},{"location":"#yapp-yet-another-python-data-pipeline","text":"yapp is a simple python data pipeline framework, inspired by ploomber . yapp allows you to quickly define pipelines to pack together your messy Python experiments and make better use of them. Code is hosted on GitHub . Warning Development is still in early stages, many things may change in the future. If you are looking for a similar tool to use right away in something resembling a production environment, just use Ploomber. yapp strives to be as simple as possible and make you focus on the correctness of your algorithms. It's developed with specific requirements in mind and built according to those: it may be the best choice for you once completed, or may be not. Basic functionality is there but there are some very rough edges to be smoothed. There are no tests and not even a proper example yet.","title":"yapp \u2014 Yet Another Python (data) Pipeline"},{"location":"#install","text":"pip install yapp-pipelines","title":"Install"},{"location":"#usage","text":"To use yapp-pipelines you need two things: A directory containing your code A pipelines.yml file inside it Pipelines configuration is defined in a pipelines.yml , it contains all the steps to perform in each of your pipelines and all the needed resources. A Pipeline is made up of Jobs. A Job represents a step of the pipeline, it takes inputs as parameters and returns a dict of outputs. A Job can a class that extends yapp.Job , or just a function returning a dictionary. The configuration defines all the dependencies of every Job in the Pipeline. They are resolved and then run one at the time (even if it may be possible to run them in parallel, this is a willingly design choice). Pipelines can have hooks to perform specific task before or after each task (such as updating some kind of status monitor) For a complete overview on how to define pipelines se the Configuration page . You can then run your pipeline with yapp [pipeline name] .","title":"Usage"},{"location":"#the-yapp-command","text":"You can run a pipeline using the yapp command: yapp [-h] [-p [PATH]] [-d] pipeline yapp automatically searches for classes and functions you use in your yaml files. It searches in, in order: The pipeline directory (if it exists) Top level directory of your code yapp built-in modules The first two are relative to the current working directory or to the supplied using path or -p","title":"The yapp command"},{"location":"#example","text":"Code Structure example_project/ \u251c\u2500\u2500my_pipeline/ # pipeline directory \u2502 \u251c\u2500\u2500MainTask.py # MainTask class inside a bad CamelCase filename \u2502 \u251c\u2500\u2500another_task.py # AnotherTask inside a better snake_case filename \u2502 \u2514\u2500\u2500... \u251c\u2500\u2500data/ \u2502 \u251c\u2500\u2500raw.csv # Input data \u2502 \u2514\u2500\u2500... \u2514\u2500\u2500pipelines.yml pipelines.yml my_pipeline : # define a new pipeline inputs : - from : file.CsvInput # use input from CSV files wih : directory : \"./data\" expose : - use : \"raw.csv\" # Read \"raw.csv\"\u2026 as : raw_data # \u2026and use it as the raw_data input steps : - run : MainTask - run : AnotherTask after : MainTask # AnotherTask must run only after MainTask","title":"Example"},{"location":"configuration/","text":"Configuration Configuration is done using a YAML file, pipelines.yml . Yapp automatically searches for it inside current working directory or in the specified path Top level keys are names of pipelines or +all , used to define a global configuration. Warning pipelines.yml specification can change anytime for 0.x.x versions. Note I wanted to write down a proper YAML schema at first but ended up with this quick notation, mainly 'cause I'm lazy. Every tag enclosed in \"< >\" is a generic placeholder, which type is later specified. version 0.1.x $pipeline : inputs : # optional - from : <adapter> with : <params> # optional expose : # optional - use : <source> as : <input(s) name(s)> outputs : # optional - to : <adapter> with : <params> # optional hooks : # optional - run : <hook> on : pipeline_start - run : <hook> on : job_start - run : <hook> on : job_finish - run : <hook> on : pipeline_finish steps : # required - run : <step> after : <step> with : <params> <adapter> : str referring to the InputAdapter class <params> : dict with the args to be passed as arguments to __init__ <hook> : str referring to the hook function <source> : str containing the key to pass to the get method <step> : str referring to the Job class or function for the job str used as <adapter> , <hook> and <step> should be valid Python module strings. Pipeline fields A pipeline can specify the following attributes, the only strictly required is steps . steps Used to define the jobs that make up the pipeline and their dependencies. Contains a list, each element represents a Job and its dependencies. For each element the following fields are valid: run after with inputs Used to define input sources. Contains a list, each element represents an InputAdapter and its required arguments. Also Contains a list of inputs to be exposed from that InputAdapter object defined. For each element the following fields are valid: from with expose use as outputs Used to define outputs to write results to. Contains a list, each element represents an OutputAdapter and its required arguments. For each element the following fields are valid: to with hooks Used to define the hooks to perform at specific events. Contains a list, each element represents a hook. For each element the following fields are valid: run on Special types Types defined by yapp that can be used in pipelines.yml : !env Reads an environment variable. If there is a DATA_DIR environment variable, to which is assigned the value ../data/latest , the following: !env DATA_DIR is automatically replaced with ../data/latest .","title":"Configuration"},{"location":"configuration/#configuration","text":"Configuration is done using a YAML file, pipelines.yml . Yapp automatically searches for it inside current working directory or in the specified path Top level keys are names of pipelines or +all , used to define a global configuration. Warning pipelines.yml specification can change anytime for 0.x.x versions. Note I wanted to write down a proper YAML schema at first but ended up with this quick notation, mainly 'cause I'm lazy. Every tag enclosed in \"< >\" is a generic placeholder, which type is later specified. version 0.1.x $pipeline : inputs : # optional - from : <adapter> with : <params> # optional expose : # optional - use : <source> as : <input(s) name(s)> outputs : # optional - to : <adapter> with : <params> # optional hooks : # optional - run : <hook> on : pipeline_start - run : <hook> on : job_start - run : <hook> on : job_finish - run : <hook> on : pipeline_finish steps : # required - run : <step> after : <step> with : <params> <adapter> : str referring to the InputAdapter class <params> : dict with the args to be passed as arguments to __init__ <hook> : str referring to the hook function <source> : str containing the key to pass to the get method <step> : str referring to the Job class or function for the job str used as <adapter> , <hook> and <step> should be valid Python module strings.","title":"Configuration"},{"location":"configuration/#pipeline-fields","text":"A pipeline can specify the following attributes, the only strictly required is steps .","title":"Pipeline fields"},{"location":"configuration/#steps","text":"Used to define the jobs that make up the pipeline and their dependencies. Contains a list, each element represents a Job and its dependencies. For each element the following fields are valid: run after with","title":"steps"},{"location":"configuration/#inputs","text":"Used to define input sources. Contains a list, each element represents an InputAdapter and its required arguments. Also Contains a list of inputs to be exposed from that InputAdapter object defined. For each element the following fields are valid: from with expose use as","title":"inputs"},{"location":"configuration/#outputs","text":"Used to define outputs to write results to. Contains a list, each element represents an OutputAdapter and its required arguments. For each element the following fields are valid: to with","title":"outputs"},{"location":"configuration/#hooks","text":"Used to define the hooks to perform at specific events. Contains a list, each element represents a hook. For each element the following fields are valid: run on","title":"hooks"},{"location":"configuration/#special-types","text":"Types defined by yapp that can be used in pipelines.yml :","title":"Special types"},{"location":"configuration/#env","text":"Reads an environment variable. If there is a DATA_DIR environment variable, to which is assigned the value ../data/latest , the following: !env DATA_DIR is automatically replaced with ../data/latest .","title":"!env"},{"location":"reference/","text":"Reference yapp.core yapp core classes AttrDict Bases: dict Extends dict so that elements can be accessed as attributes __init__ ( other = None , ** kwargs ) Create a new AttrDict Parameters: Name Type Description Default self None, mapping , iterable required recursive_convert ( obj ) staticmethod Makes all dict inside obj AttrDict InputAdapter Bases: ABC Abstract Input Adapter An input adapter represents a type of input from a specific source get ( key ) abstractmethod Returns the requested input Inputs Bases: dict Inputs implementation (just dict with some utility methods) expose ( source , internal_name , name ) Expose input attribute using another name register ( name , adapter ) New input adapter (just a new Item) Job Bases: ABC Job represents a step in our pipeline completed () property True if job successfully run, False otherwise config () property Shortcut for self.pipeline.config execute ( * inputs , ** params ) abstractmethod Job entrypoint name () property Helper to return the name of the class Monitor Pipeline status monitoring class Wrapper class used to group hooks, just define your hooks as class method and they will be automatically called when needed OutputAdapter Bases: ABC Abstract output Adapter An output adapter represents a specific output destination empty ( job_name ) Override this if you wish to save something when a Job returns nothing, Leave as it is you prefer ignoring it. Parameters: Name Type Description Default job_name str Name of the job returning None required name () property Helper to return the name of the class save ( key , data ) abstractmethod Save intermediate data here Parameters: Name Type Description Default key str Key is the name used as key in the returned dict from the Job, or if it didn't return a dictionary, the Job's name. required data dict | Any Data returned from Job execution required save_result ( key , data ) Save final result here Leave it as it is you just use save Pipeline yapp Pipeline object Pipeline implementation. Collects jobs, hooks and input and output adapter and runs the pipeline. Attributes: Name Type Description OK_LOGLEVEL int Loglevel to use for pipeline and jobs completed execution status messages VALID_HOOKS list list of valid hooks that can be used in a pipeline __nested_timed_calls int level of nested calls to timed , used to enhance logging __call__ ( save_results = None ) Pipeline entrypoint Sets up inputs, outputs and config (if specified) and runs the pipeline __init__ ( job_list , name = '' , inputs = None , outputs = None , monitor = None , ** hooks ) init . Parameters: Name Type Description Default job_list Sequence [ type [ Job ]] List of Jobs classes to run (in correct order) inside the pipeline required name str Pipeline name '' inputs Union [ Inputs , None] Inputs for the pipeline None outputs Union [ Sequence [ type [ OutputAdapter ]], Set [ type [ OutputAdapter ]], type [ OutputAdapter ], None] Outputs for the pipeline None monitor Union [ Monitor , None] Monitor for the pipeline None **hooks Hooks to attach to the pipeline {} _run () Runs all Pipeline's jobs _run_job ( job ) Execution of a single job completed () property True if pipeline successfully run, False otherwise config () property Shortcut for configuration from inputs job_name () property Shortcut for self.current_job.name which handles no current_job run_hook ( hook_name ) Run all hooks for current event A hook is just a function taking a pipeline as single argument Parameters: Name Type Description Default hook_name str name of the hook to run (\"on_pipeline_start\", \"on_job_start\", etc.) required save_output ( name , data , results = False ) Save data to each output adapter Parameters: Name Type Description Default name str name to pass to the output adapters when saving the data required data Any data to save required timed ( typename , name , func , * args , _update_object = None , ** kwargs ) Runs a timed execution of a function, logging times The first two parameters are used to specify the type and name of the entity to run. Parameters: Name Type Description Default typename str name of the type of the component to run (\"pipeline\", \"job\", \"hook\", etc.) required name str name of the component to run required func callable function to run required *args () **kwargs {} Returns: Type Description (Any) The output of provided function attr_dict AttrDict Bases: dict Extends dict so that elements can be accessed as attributes __init__ ( other = None , ** kwargs ) Create a new AttrDict Parameters: Name Type Description Default self None, mapping , iterable required recursive_convert ( obj ) staticmethod Makes all dict inside obj AttrDict errors ConfigurationError Bases: YappFatalError Exception raised when an invalid configuration file is found EmptyConfiguration Bases: YappFatalError Exception raised when an empty configuration file is found ImportedCodeFailed Bases: YappFatalError Exception raised when a module is found bu importing it fails MissingConfiguration Bases: YappFatalError Exception raised when no configuration file is found MissingEnv Bases: YappFatalError Exception raised when an environment variable requested in config is not defined MissingPipeline Bases: YappFatalError Exception raised when users requests a pipeline name not in pipelines.yml YappFatalError Bases: RuntimeError , ABC Generic fatal exception log () abstractmethod Used to log specific error messages log_and_exit () Calls log and exit with the relevan error exit_code input_adapter InputAdapter Bases: ABC Abstract Input Adapter An input adapter represents a type of input from a specific source get ( key ) abstractmethod Returns the requested input inputs Inputs Bases: dict Inputs implementation (just dict with some utility methods) expose ( source , internal_name , name ) Expose input attribute using another name register ( name , adapter ) New input adapter (just a new Item) job Job Bases: ABC Job represents a step in our pipeline completed () property True if job successfully run, False otherwise config () property Shortcut for self.pipeline.config execute ( * inputs , ** params ) abstractmethod Job entrypoint name () property Helper to return the name of the class monitor Monitor Pipeline status monitoring class Wrapper class used to group hooks, just define your hooks as class method and they will be automatically called when needed output_adapter OutputAdapter Bases: ABC Abstract output Adapter An output adapter represents a specific output destination empty ( job_name ) Override this if you wish to save something when a Job returns nothing, Leave as it is you prefer ignoring it. Parameters: Name Type Description Default job_name str Name of the job returning None required name () property Helper to return the name of the class save ( key , data ) abstractmethod Save intermediate data here Parameters: Name Type Description Default key str Key is the name used as key in the returned dict from the Job, or if it didn't return a dictionary, the Job's name. required data dict | Any Data returned from Job execution required save_result ( key , data ) Save final result here Leave it as it is you just use save pipeline Pipeline yapp Pipeline object Pipeline implementation. Collects jobs, hooks and input and output adapter and runs the pipeline. Attributes: Name Type Description OK_LOGLEVEL int Loglevel to use for pipeline and jobs completed execution status messages VALID_HOOKS list list of valid hooks that can be used in a pipeline __nested_timed_calls int level of nested calls to timed , used to enhance logging __call__ ( save_results = None ) Pipeline entrypoint Sets up inputs, outputs and config (if specified) and runs the pipeline __init__ ( job_list , name = '' , inputs = None , outputs = None , monitor = None , ** hooks ) init . Parameters: Name Type Description Default job_list Sequence [ type [ Job ]] List of Jobs classes to run (in correct order) inside the pipeline required name str Pipeline name '' inputs Union [ Inputs , None] Inputs for the pipeline None outputs Union [ Sequence [ type [ OutputAdapter ]], Set [ type [ OutputAdapter ]], type [ OutputAdapter ], None] Outputs for the pipeline None monitor Union [ Monitor , None] Monitor for the pipeline None **hooks Hooks to attach to the pipeline {} _run () Runs all Pipeline's jobs _run_job ( job ) Execution of a single job completed () property True if pipeline successfully run, False otherwise config () property Shortcut for configuration from inputs job_name () property Shortcut for self.current_job.name which handles no current_job run_hook ( hook_name ) Run all hooks for current event A hook is just a function taking a pipeline as single argument Parameters: Name Type Description Default hook_name str name of the hook to run (\"on_pipeline_start\", \"on_job_start\", etc.) required save_output ( name , data , results = False ) Save data to each output adapter Parameters: Name Type Description Default name str name to pass to the output adapters when saving the data required data Any data to save required timed ( typename , name , func , * args , _update_object = None , ** kwargs ) Runs a timed execution of a function, logging times The first two parameters are used to specify the type and name of the entity to run. Parameters: Name Type Description Default typename str name of the type of the component to run (\"pipeline\", \"job\", \"hook\", etc.) required name str name of the component to run required func callable function to run required *args () **kwargs {} Returns: Type Description (Any) The output of provided function enforce_list ( value ) Makes sure the argument can be treated as a list yapp.cli yapp cli parsing main () yapp cli entrypoint logs LogFormatter Bases: logging . Formatter Custom LogFormatter, probably not the best way at all to do this but was fun doing it this way. get_color ( loglevel = None ) Returns the color escape characters to print add_logging_level ( level_name , level_num , method_name = None ) Comprehensively adds a new logging level to the logging module and the currently configured logging class. level_name becomes an attribute of the logging module with the value level_num . method_name becomes a convenience method for both logging itself and the class returned by logging.getLoggerClass() (usually just logging.Logger ). If method_name is not specified, level_name.lower() is used. To avoid accidental clobberings of existing attributes, this method will raise an AttributeError if the level name is already an attribute of the logging module or if the method name is already present Taken from this great answer on StackOverflow Example add_logging_level('TRACE', logging.DEBUG - 5) logging.getLogger( name ).setLevel(\"TRACE\") logging.getLogger( name ).trace('that worked') logging.trace('so did this') logging.TRACE 5 setup_logging ( loglevel , color = False , logfile = '' , show_lineno = False ) Setup logging for yapp parsing ConfigParser Parses config files and build a pipeline accordingly build_hooks ( cfg_hooks ) Sets up hooks from hooks field in YAML files build_inputs ( cfg_inputs , config = None ) Sets up inputs from inputs and expose fields in YAML files build_job ( step , params ) Create Job given pipeline and step name build_monitor ( cfg_monitor ) Sets up monitor from monitor field in YAML files build_new_job_class ( step , module , func_name , params ) Build new Job subclass at runtime build_outputs ( cfg_outputs ) Sets up outputs from outputs field in YAML files build_pipeline ( pipeline_cfg , inputs = None , outputs = None , hooks = None , monitor = None ) Creates pipeline from pipeline and config definition dicts create_adapter ( adapter_name , params ) Loads the relevant module and instantiates an adapter from it do_validation ( pipelines_yaml ) Performs validation on a dict read from a pipelines.yml file Parameters: Name Type Description Default pipelines_yaml dict pipelines_yaml required load_module ( module_name ) Loads a python module from a .py file or yapp modules make_hook ( single_hook ) Create a single hook from its dict Configuration make_input ( single_input ) Create a single input from its dict Configuration make_output ( single_output ) Create a single output from its dict Configuration parse ( skip_validation = False ) Reads and parses pipelines.yml, creates a pipeline object switch_workdir ( workdir = None ) Switches to the pipeline workdir that jobs and hooks expect camel_to_snake ( name ) Returns snake_case version of a CamelCase string do_nothing_constructor ( self , node ) Constructor just returning the string for the node env_constructor ( loader , node ) Conctructor to automatically look up for env variables yaml_read ( path ) Read YAML from path validation ErrorHandler Bases: BasicErrorHandler Cerberus custom ErrorHandler to print config errors the way I want check_code_reference ( field , value , error ) Check if a string can be a valid python module or function reference validate ( definitions ) Validate schema for definitions from YAML file yapp.adapters yapp input and ouput adapters file CsvInput Bases: InputAdapter CSV Input adapter An input adapter for CSV files, input is read into a pandas DataFrame pandas FunctionWrapperInputAdapter Bases: InputAdapter Very hacky generic input adapter pgsql PgSqlInput Bases: SqlInput Very simple PostgreSQL input adapter PgSqlOutput Bases: SqlOutput Very simple PostgreSQL ouput adapter make_pgsql_connection ( username , password , host , port , database ) Create PostgreSQL connection using SQLAlchemy create_engine Parameters: Name Type Description Default username required password required host required port required database required snowflake SnowflakeInput Bases: SqlInput Very simple PostgreSQL input adapter sql SqlInput Bases: InputAdapter SQL Input adapter An input adapter for SQL databases, input is read into a pandas DataFrame SqlOutput Bases: OutputAdapter SQL output adapter Output adapter for SQL databases, a pandas DataFrame is written to a table utils DummyInput Bases: InputAdapter Dummy input adapter that always returns an empty DataFrame DummyOutput Bases: OutputAdapter Dummy output adapter that prints data it should save","title":"Reference"},{"location":"reference/#reference","text":"","title":"Reference"},{"location":"reference/#yapp.core","text":"yapp core classes","title":"core"},{"location":"reference/#yapp.core.AttrDict","text":"Bases: dict Extends dict so that elements can be accessed as attributes","title":"AttrDict"},{"location":"reference/#yapp.core.attr_dict.AttrDict.__init__","text":"Create a new AttrDict Parameters: Name Type Description Default self None, mapping , iterable required","title":"__init__()"},{"location":"reference/#yapp.core.attr_dict.AttrDict.recursive_convert","text":"Makes all dict inside obj AttrDict","title":"recursive_convert()"},{"location":"reference/#yapp.core.InputAdapter","text":"Bases: ABC Abstract Input Adapter An input adapter represents a type of input from a specific source","title":"InputAdapter"},{"location":"reference/#yapp.core.input_adapter.InputAdapter.get","text":"Returns the requested input","title":"get()"},{"location":"reference/#yapp.core.Inputs","text":"Bases: dict Inputs implementation (just dict with some utility methods)","title":"Inputs"},{"location":"reference/#yapp.core.inputs.Inputs.expose","text":"Expose input attribute using another name","title":"expose()"},{"location":"reference/#yapp.core.inputs.Inputs.register","text":"New input adapter (just a new Item)","title":"register()"},{"location":"reference/#yapp.core.Job","text":"Bases: ABC Job represents a step in our pipeline","title":"Job"},{"location":"reference/#yapp.core.job.Job.completed","text":"True if job successfully run, False otherwise","title":"completed()"},{"location":"reference/#yapp.core.job.Job.config","text":"Shortcut for self.pipeline.config","title":"config()"},{"location":"reference/#yapp.core.job.Job.execute","text":"Job entrypoint","title":"execute()"},{"location":"reference/#yapp.core.job.Job.name","text":"Helper to return the name of the class","title":"name()"},{"location":"reference/#yapp.core.Monitor","text":"Pipeline status monitoring class Wrapper class used to group hooks, just define your hooks as class method and they will be automatically called when needed","title":"Monitor"},{"location":"reference/#yapp.core.OutputAdapter","text":"Bases: ABC Abstract output Adapter An output adapter represents a specific output destination","title":"OutputAdapter"},{"location":"reference/#yapp.core.output_adapter.OutputAdapter.empty","text":"Override this if you wish to save something when a Job returns nothing, Leave as it is you prefer ignoring it. Parameters: Name Type Description Default job_name str Name of the job returning None required","title":"empty()"},{"location":"reference/#yapp.core.output_adapter.OutputAdapter.name","text":"Helper to return the name of the class","title":"name()"},{"location":"reference/#yapp.core.output_adapter.OutputAdapter.save","text":"Save intermediate data here Parameters: Name Type Description Default key str Key is the name used as key in the returned dict from the Job, or if it didn't return a dictionary, the Job's name. required data dict | Any Data returned from Job execution required","title":"save()"},{"location":"reference/#yapp.core.output_adapter.OutputAdapter.save_result","text":"Save final result here Leave it as it is you just use save","title":"save_result()"},{"location":"reference/#yapp.core.Pipeline","text":"yapp Pipeline object Pipeline implementation. Collects jobs, hooks and input and output adapter and runs the pipeline. Attributes: Name Type Description OK_LOGLEVEL int Loglevel to use for pipeline and jobs completed execution status messages VALID_HOOKS list list of valid hooks that can be used in a pipeline __nested_timed_calls int level of nested calls to timed , used to enhance logging","title":"Pipeline"},{"location":"reference/#yapp.core.pipeline.Pipeline.__call__","text":"Pipeline entrypoint Sets up inputs, outputs and config (if specified) and runs the pipeline","title":"__call__()"},{"location":"reference/#yapp.core.pipeline.Pipeline.__init__","text":"init . Parameters: Name Type Description Default job_list Sequence [ type [ Job ]] List of Jobs classes to run (in correct order) inside the pipeline required name str Pipeline name '' inputs Union [ Inputs , None] Inputs for the pipeline None outputs Union [ Sequence [ type [ OutputAdapter ]], Set [ type [ OutputAdapter ]], type [ OutputAdapter ], None] Outputs for the pipeline None monitor Union [ Monitor , None] Monitor for the pipeline None **hooks Hooks to attach to the pipeline {}","title":"__init__()"},{"location":"reference/#yapp.core.pipeline.Pipeline._run","text":"Runs all Pipeline's jobs","title":"_run()"},{"location":"reference/#yapp.core.pipeline.Pipeline._run_job","text":"Execution of a single job","title":"_run_job()"},{"location":"reference/#yapp.core.pipeline.Pipeline.completed","text":"True if pipeline successfully run, False otherwise","title":"completed()"},{"location":"reference/#yapp.core.pipeline.Pipeline.config","text":"Shortcut for configuration from inputs","title":"config()"},{"location":"reference/#yapp.core.pipeline.Pipeline.job_name","text":"Shortcut for self.current_job.name which handles no current_job","title":"job_name()"},{"location":"reference/#yapp.core.pipeline.Pipeline.run_hook","text":"Run all hooks for current event A hook is just a function taking a pipeline as single argument Parameters: Name Type Description Default hook_name str name of the hook to run (\"on_pipeline_start\", \"on_job_start\", etc.) required","title":"run_hook()"},{"location":"reference/#yapp.core.pipeline.Pipeline.save_output","text":"Save data to each output adapter Parameters: Name Type Description Default name str name to pass to the output adapters when saving the data required data Any data to save required","title":"save_output()"},{"location":"reference/#yapp.core.pipeline.Pipeline.timed","text":"Runs a timed execution of a function, logging times The first two parameters are used to specify the type and name of the entity to run. Parameters: Name Type Description Default typename str name of the type of the component to run (\"pipeline\", \"job\", \"hook\", etc.) required name str name of the component to run required func callable function to run required *args () **kwargs {} Returns: Type Description (Any) The output of provided function","title":"timed()"},{"location":"reference/#yapp.core.attr_dict","text":"","title":"attr_dict"},{"location":"reference/#yapp.core.attr_dict.AttrDict","text":"Bases: dict Extends dict so that elements can be accessed as attributes","title":"AttrDict"},{"location":"reference/#yapp.core.attr_dict.AttrDict.__init__","text":"Create a new AttrDict Parameters: Name Type Description Default self None, mapping , iterable required","title":"__init__()"},{"location":"reference/#yapp.core.attr_dict.AttrDict.recursive_convert","text":"Makes all dict inside obj AttrDict","title":"recursive_convert()"},{"location":"reference/#yapp.core.errors","text":"","title":"errors"},{"location":"reference/#yapp.core.errors.ConfigurationError","text":"Bases: YappFatalError Exception raised when an invalid configuration file is found","title":"ConfigurationError"},{"location":"reference/#yapp.core.errors.EmptyConfiguration","text":"Bases: YappFatalError Exception raised when an empty configuration file is found","title":"EmptyConfiguration"},{"location":"reference/#yapp.core.errors.ImportedCodeFailed","text":"Bases: YappFatalError Exception raised when a module is found bu importing it fails","title":"ImportedCodeFailed"},{"location":"reference/#yapp.core.errors.MissingConfiguration","text":"Bases: YappFatalError Exception raised when no configuration file is found","title":"MissingConfiguration"},{"location":"reference/#yapp.core.errors.MissingEnv","text":"Bases: YappFatalError Exception raised when an environment variable requested in config is not defined","title":"MissingEnv"},{"location":"reference/#yapp.core.errors.MissingPipeline","text":"Bases: YappFatalError Exception raised when users requests a pipeline name not in pipelines.yml","title":"MissingPipeline"},{"location":"reference/#yapp.core.errors.YappFatalError","text":"Bases: RuntimeError , ABC Generic fatal exception","title":"YappFatalError"},{"location":"reference/#yapp.core.errors.YappFatalError.log","text":"Used to log specific error messages","title":"log()"},{"location":"reference/#yapp.core.errors.YappFatalError.log_and_exit","text":"Calls log and exit with the relevan error exit_code","title":"log_and_exit()"},{"location":"reference/#yapp.core.input_adapter","text":"","title":"input_adapter"},{"location":"reference/#yapp.core.input_adapter.InputAdapter","text":"Bases: ABC Abstract Input Adapter An input adapter represents a type of input from a specific source","title":"InputAdapter"},{"location":"reference/#yapp.core.input_adapter.InputAdapter.get","text":"Returns the requested input","title":"get()"},{"location":"reference/#yapp.core.inputs","text":"","title":"inputs"},{"location":"reference/#yapp.core.inputs.Inputs","text":"Bases: dict Inputs implementation (just dict with some utility methods)","title":"Inputs"},{"location":"reference/#yapp.core.inputs.Inputs.expose","text":"Expose input attribute using another name","title":"expose()"},{"location":"reference/#yapp.core.inputs.Inputs.register","text":"New input adapter (just a new Item)","title":"register()"},{"location":"reference/#yapp.core.job","text":"","title":"job"},{"location":"reference/#yapp.core.job.Job","text":"Bases: ABC Job represents a step in our pipeline","title":"Job"},{"location":"reference/#yapp.core.job.Job.completed","text":"True if job successfully run, False otherwise","title":"completed()"},{"location":"reference/#yapp.core.job.Job.config","text":"Shortcut for self.pipeline.config","title":"config()"},{"location":"reference/#yapp.core.job.Job.execute","text":"Job entrypoint","title":"execute()"},{"location":"reference/#yapp.core.job.Job.name","text":"Helper to return the name of the class","title":"name()"},{"location":"reference/#yapp.core.monitor","text":"","title":"monitor"},{"location":"reference/#yapp.core.monitor.Monitor","text":"Pipeline status monitoring class Wrapper class used to group hooks, just define your hooks as class method and they will be automatically called when needed","title":"Monitor"},{"location":"reference/#yapp.core.output_adapter","text":"","title":"output_adapter"},{"location":"reference/#yapp.core.output_adapter.OutputAdapter","text":"Bases: ABC Abstract output Adapter An output adapter represents a specific output destination","title":"OutputAdapter"},{"location":"reference/#yapp.core.output_adapter.OutputAdapter.empty","text":"Override this if you wish to save something when a Job returns nothing, Leave as it is you prefer ignoring it. Parameters: Name Type Description Default job_name str Name of the job returning None required","title":"empty()"},{"location":"reference/#yapp.core.output_adapter.OutputAdapter.name","text":"Helper to return the name of the class","title":"name()"},{"location":"reference/#yapp.core.output_adapter.OutputAdapter.save","text":"Save intermediate data here Parameters: Name Type Description Default key str Key is the name used as key in the returned dict from the Job, or if it didn't return a dictionary, the Job's name. required data dict | Any Data returned from Job execution required","title":"save()"},{"location":"reference/#yapp.core.output_adapter.OutputAdapter.save_result","text":"Save final result here Leave it as it is you just use save","title":"save_result()"},{"location":"reference/#yapp.core.pipeline","text":"","title":"pipeline"},{"location":"reference/#yapp.core.pipeline.Pipeline","text":"yapp Pipeline object Pipeline implementation. Collects jobs, hooks and input and output adapter and runs the pipeline. Attributes: Name Type Description OK_LOGLEVEL int Loglevel to use for pipeline and jobs completed execution status messages VALID_HOOKS list list of valid hooks that can be used in a pipeline __nested_timed_calls int level of nested calls to timed , used to enhance logging","title":"Pipeline"},{"location":"reference/#yapp.core.pipeline.Pipeline.__call__","text":"Pipeline entrypoint Sets up inputs, outputs and config (if specified) and runs the pipeline","title":"__call__()"},{"location":"reference/#yapp.core.pipeline.Pipeline.__init__","text":"init . Parameters: Name Type Description Default job_list Sequence [ type [ Job ]] List of Jobs classes to run (in correct order) inside the pipeline required name str Pipeline name '' inputs Union [ Inputs , None] Inputs for the pipeline None outputs Union [ Sequence [ type [ OutputAdapter ]], Set [ type [ OutputAdapter ]], type [ OutputAdapter ], None] Outputs for the pipeline None monitor Union [ Monitor , None] Monitor for the pipeline None **hooks Hooks to attach to the pipeline {}","title":"__init__()"},{"location":"reference/#yapp.core.pipeline.Pipeline._run","text":"Runs all Pipeline's jobs","title":"_run()"},{"location":"reference/#yapp.core.pipeline.Pipeline._run_job","text":"Execution of a single job","title":"_run_job()"},{"location":"reference/#yapp.core.pipeline.Pipeline.completed","text":"True if pipeline successfully run, False otherwise","title":"completed()"},{"location":"reference/#yapp.core.pipeline.Pipeline.config","text":"Shortcut for configuration from inputs","title":"config()"},{"location":"reference/#yapp.core.pipeline.Pipeline.job_name","text":"Shortcut for self.current_job.name which handles no current_job","title":"job_name()"},{"location":"reference/#yapp.core.pipeline.Pipeline.run_hook","text":"Run all hooks for current event A hook is just a function taking a pipeline as single argument Parameters: Name Type Description Default hook_name str name of the hook to run (\"on_pipeline_start\", \"on_job_start\", etc.) required","title":"run_hook()"},{"location":"reference/#yapp.core.pipeline.Pipeline.save_output","text":"Save data to each output adapter Parameters: Name Type Description Default name str name to pass to the output adapters when saving the data required data Any data to save required","title":"save_output()"},{"location":"reference/#yapp.core.pipeline.Pipeline.timed","text":"Runs a timed execution of a function, logging times The first two parameters are used to specify the type and name of the entity to run. Parameters: Name Type Description Default typename str name of the type of the component to run (\"pipeline\", \"job\", \"hook\", etc.) required name str name of the component to run required func callable function to run required *args () **kwargs {} Returns: Type Description (Any) The output of provided function","title":"timed()"},{"location":"reference/#yapp.core.pipeline.enforce_list","text":"Makes sure the argument can be treated as a list","title":"enforce_list()"},{"location":"reference/#yapp.cli","text":"yapp cli parsing","title":"cli"},{"location":"reference/#yapp.cli.main","text":"yapp cli entrypoint","title":"main()"},{"location":"reference/#yapp.cli.logs","text":"","title":"logs"},{"location":"reference/#yapp.cli.logs.LogFormatter","text":"Bases: logging . Formatter Custom LogFormatter, probably not the best way at all to do this but was fun doing it this way.","title":"LogFormatter"},{"location":"reference/#yapp.cli.logs.LogFormatter.get_color","text":"Returns the color escape characters to print","title":"get_color()"},{"location":"reference/#yapp.cli.logs.add_logging_level","text":"Comprehensively adds a new logging level to the logging module and the currently configured logging class. level_name becomes an attribute of the logging module with the value level_num . method_name becomes a convenience method for both logging itself and the class returned by logging.getLoggerClass() (usually just logging.Logger ). If method_name is not specified, level_name.lower() is used. To avoid accidental clobberings of existing attributes, this method will raise an AttributeError if the level name is already an attribute of the logging module or if the method name is already present Taken from this great answer on StackOverflow","title":"add_logging_level()"},{"location":"reference/#yapp.cli.logs.add_logging_level--example","text":"add_logging_level('TRACE', logging.DEBUG - 5) logging.getLogger( name ).setLevel(\"TRACE\") logging.getLogger( name ).trace('that worked') logging.trace('so did this') logging.TRACE 5","title":"Example"},{"location":"reference/#yapp.cli.logs.setup_logging","text":"Setup logging for yapp","title":"setup_logging()"},{"location":"reference/#yapp.cli.parsing","text":"","title":"parsing"},{"location":"reference/#yapp.cli.parsing.ConfigParser","text":"Parses config files and build a pipeline accordingly","title":"ConfigParser"},{"location":"reference/#yapp.cli.parsing.ConfigParser.build_hooks","text":"Sets up hooks from hooks field in YAML files","title":"build_hooks()"},{"location":"reference/#yapp.cli.parsing.ConfigParser.build_inputs","text":"Sets up inputs from inputs and expose fields in YAML files","title":"build_inputs()"},{"location":"reference/#yapp.cli.parsing.ConfigParser.build_job","text":"Create Job given pipeline and step name","title":"build_job()"},{"location":"reference/#yapp.cli.parsing.ConfigParser.build_monitor","text":"Sets up monitor from monitor field in YAML files","title":"build_monitor()"},{"location":"reference/#yapp.cli.parsing.ConfigParser.build_new_job_class","text":"Build new Job subclass at runtime","title":"build_new_job_class()"},{"location":"reference/#yapp.cli.parsing.ConfigParser.build_outputs","text":"Sets up outputs from outputs field in YAML files","title":"build_outputs()"},{"location":"reference/#yapp.cli.parsing.ConfigParser.build_pipeline","text":"Creates pipeline from pipeline and config definition dicts","title":"build_pipeline()"},{"location":"reference/#yapp.cli.parsing.ConfigParser.create_adapter","text":"Loads the relevant module and instantiates an adapter from it","title":"create_adapter()"},{"location":"reference/#yapp.cli.parsing.ConfigParser.do_validation","text":"Performs validation on a dict read from a pipelines.yml file Parameters: Name Type Description Default pipelines_yaml dict pipelines_yaml required","title":"do_validation()"},{"location":"reference/#yapp.cli.parsing.ConfigParser.load_module","text":"Loads a python module from a .py file or yapp modules","title":"load_module()"},{"location":"reference/#yapp.cli.parsing.ConfigParser.make_hook","text":"Create a single hook from its dict Configuration","title":"make_hook()"},{"location":"reference/#yapp.cli.parsing.ConfigParser.make_input","text":"Create a single input from its dict Configuration","title":"make_input()"},{"location":"reference/#yapp.cli.parsing.ConfigParser.make_output","text":"Create a single output from its dict Configuration","title":"make_output()"},{"location":"reference/#yapp.cli.parsing.ConfigParser.parse","text":"Reads and parses pipelines.yml, creates a pipeline object","title":"parse()"},{"location":"reference/#yapp.cli.parsing.ConfigParser.switch_workdir","text":"Switches to the pipeline workdir that jobs and hooks expect","title":"switch_workdir()"},{"location":"reference/#yapp.cli.parsing.camel_to_snake","text":"Returns snake_case version of a CamelCase string","title":"camel_to_snake()"},{"location":"reference/#yapp.cli.parsing.do_nothing_constructor","text":"Constructor just returning the string for the node","title":"do_nothing_constructor()"},{"location":"reference/#yapp.cli.parsing.env_constructor","text":"Conctructor to automatically look up for env variables","title":"env_constructor()"},{"location":"reference/#yapp.cli.parsing.yaml_read","text":"Read YAML from path","title":"yaml_read()"},{"location":"reference/#yapp.cli.validation","text":"","title":"validation"},{"location":"reference/#yapp.cli.validation.ErrorHandler","text":"Bases: BasicErrorHandler Cerberus custom ErrorHandler to print config errors the way I want","title":"ErrorHandler"},{"location":"reference/#yapp.cli.validation.check_code_reference","text":"Check if a string can be a valid python module or function reference","title":"check_code_reference()"},{"location":"reference/#yapp.cli.validation.validate","text":"Validate schema for definitions from YAML file","title":"validate()"},{"location":"reference/#yapp.adapters","text":"yapp input and ouput adapters","title":"adapters"},{"location":"reference/#yapp.adapters.file","text":"","title":"file"},{"location":"reference/#yapp.adapters.file.CsvInput","text":"Bases: InputAdapter CSV Input adapter An input adapter for CSV files, input is read into a pandas DataFrame","title":"CsvInput"},{"location":"reference/#yapp.adapters.pandas","text":"","title":"pandas"},{"location":"reference/#yapp.adapters.pandas.FunctionWrapperInputAdapter","text":"Bases: InputAdapter Very hacky generic input adapter","title":"FunctionWrapperInputAdapter"},{"location":"reference/#yapp.adapters.pgsql","text":"","title":"pgsql"},{"location":"reference/#yapp.adapters.pgsql.PgSqlInput","text":"Bases: SqlInput Very simple PostgreSQL input adapter","title":"PgSqlInput"},{"location":"reference/#yapp.adapters.pgsql.PgSqlOutput","text":"Bases: SqlOutput Very simple PostgreSQL ouput adapter","title":"PgSqlOutput"},{"location":"reference/#yapp.adapters.pgsql.make_pgsql_connection","text":"Create PostgreSQL connection using SQLAlchemy create_engine Parameters: Name Type Description Default username required password required host required port required database required","title":"make_pgsql_connection()"},{"location":"reference/#yapp.adapters.snowflake","text":"","title":"snowflake"},{"location":"reference/#yapp.adapters.snowflake.SnowflakeInput","text":"Bases: SqlInput Very simple PostgreSQL input adapter","title":"SnowflakeInput"},{"location":"reference/#yapp.adapters.sql","text":"","title":"sql"},{"location":"reference/#yapp.adapters.sql.SqlInput","text":"Bases: InputAdapter SQL Input adapter An input adapter for SQL databases, input is read into a pandas DataFrame","title":"SqlInput"},{"location":"reference/#yapp.adapters.sql.SqlOutput","text":"Bases: OutputAdapter SQL output adapter Output adapter for SQL databases, a pandas DataFrame is written to a table","title":"SqlOutput"},{"location":"reference/#yapp.adapters.utils","text":"","title":"utils"},{"location":"reference/#yapp.adapters.utils.DummyInput","text":"Bases: InputAdapter Dummy input adapter that always returns an empty DataFrame","title":"DummyInput"},{"location":"reference/#yapp.adapters.utils.DummyOutput","text":"Bases: OutputAdapter Dummy output adapter that prints data it should save","title":"DummyOutput"},{"location":"roadmap/","text":"Ideas for a possible evolution in 1.x.x pipelines.yml schema now looks good. There are some possible enhancements but I expect the changes to be mostly backward compatible from now on. Planned features and changes: Detailed pipelines.yml specification Aliases for inputs, outputs and steps More flexible outputs (allow defining for which steps each output should be used) A good and working example Pipeline status monitor class example Better comprehensive docstrings Better tests Allow importing from Jupyter notebooks Consider permitting repeted tasks in a single pipeline (can this be useful?) For each step, keep track of the inputs required in future steps. So that unneeded ones can be removed from memory Graph data flow between jobs","title":"Roadmap"},{"location":"roadmap/#ideas-for-a-possible-evolution-in-1xx","text":"pipelines.yml schema now looks good. There are some possible enhancements but I expect the changes to be mostly backward compatible from now on.","title":"Ideas for a possible evolution in 1.x.x"},{"location":"roadmap/#planned-features-and-changes","text":"Detailed pipelines.yml specification Aliases for inputs, outputs and steps More flexible outputs (allow defining for which steps each output should be used) A good and working example Pipeline status monitor class example Better comprehensive docstrings Better tests Allow importing from Jupyter notebooks Consider permitting repeted tasks in a single pipeline (can this be useful?) For each step, keep track of the inputs required in future steps. So that unneeded ones can be removed from memory Graph data flow between jobs","title":"Planned features and changes:"}]}